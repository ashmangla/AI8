{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lElF3o5PR6ys"
      },
      "source": [
        "# Your First RAG Application\n",
        "\n",
        "In this notebook, we'll walk you through each of the components that are involved in a simple RAG application.\n",
        "\n",
        "We won't be leveraging any fancy tools, just the OpenAI Python SDK, Numpy, and some classic Python.\n",
        "\n",
        "> NOTE: This was done with Python 3.12.3.\n",
        "\n",
        "> NOTE: There might be [compatibility issues](https://github.com/wandb/wandb/issues/7683) if you're on NVIDIA driver >552.44 As an interim solution - you can rollback your drivers to the 552.44."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CtcL8P8R6yt"
      },
      "source": [
        "## Table of Contents:\n",
        "\n",
        "- Task 1: Imports and Utilities\n",
        "- Task 2: Documents\n",
        "- Task 3: Embeddings and Vectors\n",
        "- Task 4: Prompts\n",
        "- Task 5: Retrieval Augmented Generation\n",
        "  - 🚧 Activity #1: Augment RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dz6GYilR6yt"
      },
      "source": [
        "Let's look at a rather complicated looking visual representation of a basic RAG application.\n",
        "\n",
        "<img src=\"https://i.imgur.com/vD8b016.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjmC0KFtR6yt"
      },
      "source": [
        "## Task 1: Imports and Utility\n",
        "\n",
        "We're just doing some imports and enabling `async` to work within the Jupyter environment here, nothing too crazy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z1dyrG4hR6yt"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from aimakerspace.text_utils import TextFileLoader, CharacterTextSplitter\n",
        "from aimakerspace.vectordatabase import VectorDatabase\n",
        "from aimakerspace.openai_utils.embedding import EmbeddingModel\n",
        "import asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9OrFZRnER6yt"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0jGnpQsR6yu"
      },
      "source": [
        "## Task 2: Documents\n",
        "\n",
        "We'll be concerning ourselves with this part of the flow in the following section:\n",
        "\n",
        "<img src=\"https://i.imgur.com/jTm9gjk.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SFPWvRUR6yu"
      },
      "source": [
        "### Loading Source Documents\n",
        "\n",
        "So, first things first, we need some documents to work with.\n",
        "\n",
        "While we could work directly with the `.txt` files (or whatever file-types you wanted to extend this to) we can instead do some batch processing of those documents at the beginning in order to store them in a more machine compatible format.\n",
        "\n",
        "In this case, we're going to parse our text file into a single document in memory.\n",
        "\n",
        "Let's look at the relevant bits of the `TextFileLoader` class:\n",
        "\n",
        "```python\n",
        "def load_file(self):\n",
        "        with open(self.path, \"r\", encoding=self.encoding) as f:\n",
        "            self.documents.append(f.read())\n",
        "```\n",
        "\n",
        "We're simply loading the document using the built in `open` method, and storing that output in our `self.documents` list.\n",
        "\n",
        "> NOTE: We're using blogs from PMarca (Marc Andreessen) as our sample data. This data is largely irrelevant as we want to focus on the mechanisms of RAG, which includes out data's shape and quality - but not specifically what the contents of the data are. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia2sUEuGR6yu",
        "outputId": "84937ecc-c35f-4c4a-a4ab-9da72625954c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 text document(s)\n",
            "Loaded 1 PDF document(s)\n"
          ]
        }
      ],
      "source": [
        "# Load text file\n",
        "from aimakerspace.text_utils import TextFileLoader, PDFFileLoader\n",
        "text_loader = TextFileLoader(\"data/PMarcaBlogs.txt\")\n",
        "text_documents = text_loader.load_documents()\n",
        "print(f\"Loaded {len(text_documents)} text document(s)\")\n",
        "\n",
        "# Load PDF file\n",
        "pdf_loader = PDFFileLoader(\"data/2A_Forbes.pdf\")\n",
        "pdf_documents = pdf_loader.load_documents()\n",
        "print(f\"Loaded {len(pdf_documents)} PDF document(s)\")\n",
        "\n",
        "# Documents are kept separate for individual chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV-tj5WFR6yu",
        "outputId": "674eb315-1ff3-4597-bcf5-38ece0a812ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text file preview:\n",
            "﻿\n",
            "The Pmarca Blog Archives\n",
            "(select posts from 2007-2009)\n",
            "Marc Andreessen\n",
            "copyright: Andreessen Horow\n",
            "\n",
            "==================================================\n",
            "\n",
            "PDF file preview:\n",
            "Destructive Corporate Leadership and Board Loyalty Bias: A case study of\n",
            "Michael Eisner’s long tenur\n"
          ]
        }
      ],
      "source": [
        "print(\"Text file preview:\")\n",
        "print(text_documents[0][:100])\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "print(\"PDF file preview:\")\n",
        "print(pdf_documents[0][:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHlTvCzYR6yu"
      },
      "source": [
        "### Splitting Text Into Chunks\n",
        "\n",
        "As we can see, there is one massive document.\n",
        "\n",
        "We'll want to chunk the document into smaller parts so it's easier to pass the most relevant snippets to the LLM.\n",
        "\n",
        "There is no fixed way to split/chunk documents - and you'll need to rely on some intuition as well as knowing your data *very* well in order to build the most robust system.\n",
        "\n",
        "For this toy example, we'll just split blindly on length.\n",
        "\n",
        ">There's an opportunity to clear up some terminology here, for this course we will be stick to the following:\n",
        ">\n",
        ">- \"source documents\" : The `.txt`, `.pdf`, `.html`, ..., files that make up the files and information we start with in its raw format\n",
        ">- \"document(s)\" : single (or more) text object(s)\n",
        ">- \"corpus\" : the combination of all of our documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6Voc0jR6yv"
      },
      "source": [
        "As you can imagine (though it's not specifically true in this toy example) the idea of splitting documents is to break them into managable sized chunks that retain the most relevant local context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMC4tsEmR6yv",
        "outputId": "08689c0b-57cd-4040-942a-8193e997f5cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text chunks: 373\n",
            "PDF chunks: 89\n",
            "Total chunks: 462\n"
          ]
        }
      ],
      "source": [
        "# Chunk each document separately\n",
        "text_splitter = CharacterTextSplitter()\n",
        "\n",
        "# Chunk text documents\n",
        "text_chunks = text_splitter.split_texts(text_documents)\n",
        "print(f\"Text chunks: {len(text_chunks)}\")\n",
        "\n",
        "# Chunk PDF documents  \n",
        "pdf_chunks = text_splitter.split_texts(pdf_documents)\n",
        "print(f\"PDF chunks: {len(pdf_chunks)}\")\n",
        "\n",
        "# Combine all chunks for vector database\n",
        "split_documents = text_chunks + pdf_chunks\n",
        "print(f\"Total chunks: {len(split_documents)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2wKT0WLR6yv"
      },
      "source": [
        "Let's take a look at some of the documents we've managed to split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcYMwWJoR6yv",
        "outputId": "20d69876-feca-4826-b4be-32915276987a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\ufeff\\nThe Pmarca Blog Archives\\n(select posts from 2007-2009)\\nMarc Andreessen\\ncopyright: Andreessen Horowitz\\ncover design: Jessica Hagy\\nproduced using: Pressbooks\\nContents\\nTHE PMARCA GUIDE TO STARTUPS\\nPart 1: Why not to do a startup 2\\nPart 2: When the VCs say \"no\" 10\\nPart 3: \"But I don\\'t know any VCs!\" 18\\nPart 4: The only thing that matters 25\\nPart 5: The Moby Dick theory of big companies 33\\nPart 6: How much funding is too little? Too much? 41\\nPart 7: Why a startup\\'s initial business plan doesn\\'t\\nmatter that much\\n49\\nTHE PMARCA GUIDE TO HIRING\\nPart 8: Hiring, managing, promoting, and Dring\\nexecutives\\n54\\nPart 9: How to hire a professional CEO 68\\nHow to hire the best people you\\'ve ever worked\\nwith\\n69\\nTHE PMARCA GUIDE TO BIG COMPANIES\\nPart 1: Turnaround! 82\\nPart 2: Retaining great people 86\\nTHE PMARCA GUIDE TO CAREER, PRODUCTIVITY,\\nAND SOME OTHER THINGS\\nIntroduction 97\\nPart 1: Opportunity 99\\nPart 2: Skills and education 107\\nPart 3: Where to go and why 120\\nThe Pmarca Guide to Personal Productivi']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_documents[0:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOU-RFP_R6yv"
      },
      "source": [
        "## Task 3: Embeddings and Vectors\n",
        "\n",
        "Next, we have to convert our corpus into a \"machine readable\" format as we explored in the Embedding Primer notebook.\n",
        "\n",
        "Today, we're going to talk about the actual process of creating, and then storing, these embeddings, and how we can leverage that to intelligently add context to our queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OpenAI API Key\n",
        "\n",
        "In order to access OpenAI's APIs, we'll need to provide our OpenAI API Key!\n",
        "\n",
        "You can work through the folder \"OpenAI API Key Setup\" for more information on this process if you don't already have an API Key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key = getpass(\"OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vector Database\n",
        "\n",
        "Let's set up our vector database to hold all our documents and their embeddings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDQrfAR1R6yv"
      },
      "source": [
        "While this is all baked into 1 call - we can look at some of the code that powers this process to get a better understanding:\n",
        "\n",
        "Let's look at our `VectorDatabase().__init__()`:\n",
        "\n",
        "```python\n",
        "def __init__(self, embedding_model: EmbeddingModel = None):\n",
        "        self.vectors = defaultdict(np.array)\n",
        "        self.embedding_model = embedding_model or EmbeddingModel()\n",
        "```\n",
        "\n",
        "As you can see - our vectors are merely stored as a dictionary of `np.array` objects.\n",
        "\n",
        "Secondly, our `VectorDatabase()` has a default `EmbeddingModel()` which is a wrapper for OpenAI's `text-embedding-3-small` model.\n",
        "\n",
        "> **Quick Info About `text-embedding-3-small`**:\n",
        "> - It has a context window of **8191** tokens\n",
        "> - It returns vectors with dimension **1536**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L273pRdeR6yv"
      },
      "source": [
        "#### ❓Question #1:\n",
        "\n",
        "The default embedding dimension of `text-embedding-3-small` is 1536, as noted above. \n",
        "\n",
        "1. Is there any way to modify this dimension?\n",
        "2. What technique does OpenAI use to achieve this?\n",
        "\n",
        "> NOTE: Check out this [API documentation](https://platform.openai.com/docs/api-reference/embeddings/create) for the answer to question #1.1, and [this documentation](https://platform.openai.com/docs/guides/embeddings/use-cases) for an answer to question #1.2!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ✅ Answer:\n",
        "1. Yes there is a way to modify this in text-embedding-3 and higher models\n",
        "2. OpenAI achieves this  by passing in the dimensions parameter in the API call.  I have changed the model to use as part of activity 1 and passed in the dimensions parameter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5FZY7K3R6yv"
      },
      "source": [
        "We can call the `async_get_embeddings` method of our `EmbeddingModel()` on a list of `str` and receive a list of `float` back!\n",
        "\n",
        "```python\n",
        "async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
        "        return await aget_embeddings(\n",
        "            list_of_text=list_of_text, engine=self.embeddings_model_name\n",
        "        )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSct6X0aR6yv"
      },
      "source": [
        "We cast those to `np.array` when we build our `VectorDatabase()`:\n",
        "\n",
        "```python\n",
        "async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
        "        embeddings = await self.embedding_model.async_get_embeddings(list_of_text)\n",
        "        for text, embedding in zip(list_of_text, embeddings):\n",
        "            self.insert(text, np.array(embedding))\n",
        "        return self\n",
        "```\n",
        "\n",
        "And that's all we need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from aimakerspace.openai_utils.embedding import EmbeddingModel\n",
        "# Use large model with 1024 dimensions\n",
        "embed_model = EmbeddingModel(\n",
        "    embeddings_model_name=\"text-embedding-3-large\",\n",
        "    dimensions=1024\n",
        ")\n",
        "vector_db = VectorDatabase(embedding_model=embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O4KoLbVDR6yv"
      },
      "outputs": [],
      "source": [
        "\n",
        "#vector_db = VectorDatabase()\n",
        "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZwaGvpR6yv"
      },
      "source": [
        "#### ❓Question #2:\n",
        "\n",
        "What are the benefits of using an `async` approach to collecting our embeddings?\n",
        "\n",
        "> NOTE: Determining the core difference between `async` and `sync` will be useful! If you get stuck - ask ChatGPT!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ✅ Answer:\n",
        "In general the primary difference between async and sync is that Sync is sequential, so the program wil wait for an operation to complete before commencing the next one. Hence if an I/O operation is taking a long time , any instructions after will be blocked even if they dont depend on the result of the I/O. In the case of async , multiple requests can be sent concurrently without waiting for each to finish. The prgram can work on non-dependant operations while waiting for the responses. This is called non-blocking Execution.\n",
        "The advantages of Async and why we use it to collect embeddings are :\n",
        "(1) lower Latency : You can make multiple API calls and the parallelization of these calls can reduce the latency significantly\n",
        "(2) Better Resource utilization - CPU cycles can be used for other tasks instead of idling while waiting for the operation to complete\n",
        "(3) Scalability - is the direct consequence of the above 2. Async can handle a large number of requests \n",
        "(4) Improved User Experience - direct result of shorter latency - users can see results or partial results sooner\n",
        "(5) Network Efficiency - Async can make better use of connection pooling reducing overhead. \n",
        "The drawback is that async code is more complex to write and maintain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRBdIt-xR6yw"
      },
      "source": [
        "So, to review what we've done so far in natural language:\n",
        "\n",
        "1. We load source documents\n",
        "2. We split those source documents into smaller chunks (documents)\n",
        "3. We send each of those documents to the `text-embedding-3-small` OpenAI API endpoint\n",
        "4. We store each of the text representations with the vector representations as keys/values in a dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-vWANZyR6yw"
      },
      "source": [
        "### Semantic Similarity\n",
        "\n",
        "The next step is to be able to query our `VectorDatabase()` with a `str` and have it return to us vectors and text that is most relevant from our corpus.\n",
        "\n",
        "We're going to use the following process to achieve this in our toy example:\n",
        "\n",
        "1. We need to embed our query with the same `EmbeddingModel()` as we used to construct our `VectorDatabase()`\n",
        "2. We loop through every vector in our `VectorDatabase()` and use a distance measure to compare how related they are\n",
        "3. We return a list of the top `k` closest vectors, with their text representations\n",
        "\n",
        "There's some very heavy optimization that can be done at each of these steps - but let's just focus on the basic pattern in this notebook.\n",
        "\n",
        "> We are using [cosine similarity](https://www.engati.com/glossary/cosine-similarity) as a distance metric in this example - but there are many many distance metrics you could use - like [these](https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55)\n",
        "\n",
        "> We are using a rather inefficient way of calculating relative distance between the query vector and all other vectors - there are more advanced approaches that are much more efficient, like [ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76d96uavR6yw",
        "outputId": "bbfccc31-20a2-41c7-c14d-46554a43ed2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('e of truly awful and sustained corporate performance, an\\nunscrupulous CEO can exploit the innate loyalty biases of even independent members of the\\nboard of directors to get his way. We now examine the 20 year career of Michael Eisner as\\nDisney corporation’s CEO to illustrate these issues, giving historical purchase to the\\nabstractions of management theory presented so far.\\nEmperor Eisner: A case study in the power of personal control in a corporation\\nMichael Eisner’s twenty reign at the top of Disney (from 1984-2003) provided him\\nwith the longevity necessary to ensure his personality was firmly stamped on the corporation\\nhe headed. In this section we describe how Eisner worked the corporate board and senior\\nmanagement to maintain and build his personal power and how he ultimately lost power as\\nthe Board and senior managers, some family members, lost confidence in him. We choose\\nEisner not because he is a crook, or swindler, but rather because as a icon of American\\nbusiness he better ex',\n",
              "  np.float64(0.5776929235934868)),\n",
              " ('ordingly.\\nSeventh, when hiring the executive to run your former specialty, be\\ncareful you don’t hire someone weak on purpose.\\nThis sounds silly, but you wouldn’t believe how oaen it happens.\\nThe CEO who used to be a product manager who has a weak\\nproduct management executive. The CEO who used to be in\\nsales who has a weak sales executive. The CEO who used to be\\nin marketing who has a weak marketing executive.\\nI call this the “Michael Eisner Memorial Weak Executive Problem” — aaer the CEO of Disney who had previously been a brilliant TV network executive. When he bought ABC at Disney, it\\npromptly fell to fourth place. His response? “If I had an extra\\ntwo days a week, I could turn around ABC myself.” Well, guess\\nwhat, he didn’t have an extra two days a week.\\nA CEO — or a startup founder — oaen has a hard time letting\\ngo of the function that brought him to the party. The result: you\\nhire someone weak into the executive role for that function so\\nthat you can continue to be “the man” — cons',\n",
              "  np.float64(0.5602550721747218)),\n",
              " ('lmingly produced organisations characterised by strong\\nmanagers and weak owners. Though a high degree of managerial discretion is necessary to\\nbest exploit specialist managerial talent, recent corporate governance scandals make it self-\\nevident that executive good behaviour cannot always be guaranteed as these benefits also\\ncreate incentives and opportunities for destructive managers to act in ways contrary to\\nowner’s interests. The Eisner/Disney case is then described and analysed to illustrate how\\nthis institutional set-up is capable of enabling an entrenched and hubristic CEO to dominate a\\nboard of directors and to hold onto office even after a succession of disappointing years of\\ncorporate performance, declining share prices and several widely-publicised fall-outs with\\nother senior executives and key suppliers. Though the case indicates that eventually Eisner\\nwas ousted this only happened after much corporate value had been destroyed and\\nnecessitated a vigorous campaign by key disg',\n",
              "  np.float64(0.5524194240248839))]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_db.search_by_text(\"What is the Michael Eisner Memorial Weak Executive Problem?\", k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TehsfIiKR6yw"
      },
      "source": [
        "## Task 4: Prompts\n",
        "\n",
        "In the following section, we'll be looking at the role of prompts - and how they help us to guide our application in the right direction.\n",
        "\n",
        "In this notebook, we're going to rely on the idea of \"zero-shot in-context learning\".\n",
        "\n",
        "This is a lot of words to say: \"We will ask it to perform our desired task in the prompt, and provide no examples.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpA0UveR6yw"
      },
      "source": [
        "### XYZRolePrompt\n",
        "\n",
        "Before we do that, let's stop and think a bit about how OpenAI's chat models work.\n",
        "\n",
        "We know they have roles - as is indicated in the following API [documentation](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages)\n",
        "\n",
        "There are three roles, and they function as follows (taken directly from [OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api)):\n",
        "\n",
        "- `{\"role\" : \"system\"}` : The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model’s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
        "- `{\"role\" : \"user\"}` : The user messages provide requests or comments for the assistant to respond to.\n",
        "- `{\"role\" : \"assistant\"}` : Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
        "\n",
        "The main idea is this:\n",
        "\n",
        "1. You start with a system message that outlines how the LLM should respond, what kind of behaviours you can expect from it, and more\n",
        "2. Then, you can provide a few examples in the form of \"assistant\"/\"user\" pairs\n",
        "3. Then, you prompt the model with the true \"user\" message.\n",
        "\n",
        "In this example, we'll be forgoing the 2nd step for simplicities sake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdZ2KWKSR6yw"
      },
      "source": [
        "#### Utility Functions\n",
        "\n",
        "You'll notice that we're using some utility functions from the `aimakerspace` module - let's take a peek at these and see what they're doing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFbeJDDsR6yw"
      },
      "source": [
        "##### XYZRolePrompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mojJSE3R6yw"
      },
      "source": [
        "Here we have our `system`, `user`, and `assistant` role prompts.\n",
        "\n",
        "Let's take a peek at what they look like:\n",
        "\n",
        "```python\n",
        "class BasePrompt:\n",
        "    def __init__(self, prompt):\n",
        "        \"\"\"\n",
        "        Initializes the BasePrompt object with a prompt template.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        \"\"\"\n",
        "        self.prompt = prompt\n",
        "        self._pattern = re.compile(r\"\\{([^}]+)\\}\")\n",
        "\n",
        "    def format_prompt(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Formats the prompt string using the keyword arguments provided.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: The formatted prompt string\n",
        "        \"\"\"\n",
        "        matches = self._pattern.findall(self.prompt)\n",
        "        return self.prompt.format(**{match: kwargs.get(match, \"\") for match in matches})\n",
        "\n",
        "    def get_input_variables(self):\n",
        "        \"\"\"\n",
        "        Gets the list of input variable names from the prompt string.\n",
        "\n",
        "        :return: List of input variable names\n",
        "        \"\"\"\n",
        "        return self._pattern.findall(self.prompt)\n",
        "```\n",
        "\n",
        "Then we have our `RolePrompt` which laser focuses us on the role pattern found in most API endpoints for LLMs.\n",
        "\n",
        "```python\n",
        "class RolePrompt(BasePrompt):\n",
        "    def __init__(self, prompt, role: str):\n",
        "        \"\"\"\n",
        "        Initializes the RolePrompt object with a prompt template and a role.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        :param role: The role for the message ('system', 'user', or 'assistant')\n",
        "        \"\"\"\n",
        "        super().__init__(prompt)\n",
        "        self.role = role\n",
        "\n",
        "    def create_message(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Creates a message dictionary with a role and a formatted message.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: Dictionary containing the role and the formatted message\n",
        "        \"\"\"\n",
        "        return {\"role\": self.role, \"content\": self.format_prompt(**kwargs)}\n",
        "```\n",
        "\n",
        "We'll look at how the `SystemRolePrompt` is constructed to get a better idea of how that extension works:\n",
        "\n",
        "```python\n",
        "class SystemRolePrompt(RolePrompt):\n",
        "    def __init__(self, prompt: str):\n",
        "        super().__init__(prompt, \"system\")\n",
        "```\n",
        "\n",
        "That pattern is repeated for our `UserRolePrompt` and our `AssistantRolePrompt` as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D361R6sMR6yw"
      },
      "source": [
        "##### ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJVQ2Pm8R6yw"
      },
      "source": [
        "Next we have our model, which is converted to a format analagous to libraries like LangChain and LlamaIndex.\n",
        "\n",
        "Let's take a peek at how that is constructed:\n",
        "\n",
        "```python\n",
        "class ChatOpenAI:\n",
        "    def __init__(self, model_name: str = \"gpt-4.1-mini\"):\n",
        "        self.model_name = model_name\n",
        "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if self.openai_api_key is None:\n",
        "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
        "\n",
        "    def run(self, messages, text_only: bool = True):\n",
        "        if not isinstance(messages, list):\n",
        "            raise ValueError(\"messages must be a list\")\n",
        "\n",
        "        openai.api_key = self.openai_api_key\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=self.model_name, messages=messages\n",
        "        )\n",
        "\n",
        "        if text_only:\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return response\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCU7FfhIR6yw"
      },
      "source": [
        "#### ❓ Question #3:\n",
        "\n",
        "When calling the OpenAI API - are there any ways we can achieve more reproducible outputs?\n",
        "\n",
        "> NOTE: Check out [this section](https://platform.openai.com/docs/guides/text-generation/) of the OpenAI documentation for the answer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ✅ Answer:\n",
        "To make them more reproducible we should \n",
        "(1) pin the application to specific model snapshots like gpt-5-2025-08-07\n",
        "(2) Set temperature = 0 or low amd top_p=1\n",
        "(3) Fix the seed \n",
        "(4) wrap calls that use the same settings which can be part of a config.\n",
        "(5) Build evals to measure and monitor the behaviours of prompts as we iterate or change models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5wcjMLCR6yw"
      },
      "source": [
        "### Creating and Prompting OpenAI's `gpt-4.1-mini`!\n",
        "\n",
        "Let's tie all these together and use it to prompt `gpt-4.1-mini`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WIfpIot7R6yw"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.openai_utils.prompts import (\n",
        "    UserRolePrompt,\n",
        "    SystemRolePrompt,\n",
        "    AssistantRolePrompt,\n",
        ")\n",
        "\n",
        "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
        "\n",
        "chat_openai = ChatOpenAI()\n",
        "user_prompt_template = \"{content}\"\n",
        "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
        "system_prompt_template = (\n",
        "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
        ")\n",
        "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
        "\n",
        "messages = [\n",
        "    system_role_prompt.create_message(expertise=\"Python\"),\n",
        "    user_role_prompt.create_message(\n",
        "        content=\"What is the best way to write a loop?\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = chat_openai.run(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHo7lssNR6yw",
        "outputId": "1d3823fa-bb6b-45f6-ddba-b41686388324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! The best way to write a loop in Python usually depends on what you're trying to achieve, but generally, Python's `for` loops are very clean and readable.\n",
            "\n",
            "Here's a simple example of a `for` loop that iterates over a list:\n",
            "\n",
            "```python\n",
            "fruits = ['apple', 'banana', 'cherry']\n",
            "\n",
            "for fruit in fruits:\n",
            "    print(fruit)\n",
            "```\n",
            "\n",
            "This will print each fruit one by one.\n",
            "\n",
            "If you want to loop a specific number of times, using `range()` is very common:\n",
            "\n",
            "```python\n",
            "for i in range(5):\n",
            "    print(i)\n",
            "```\n",
            "\n",
            "This prints numbers from 0 to 4.\n",
            "\n",
            "For cases where you're not certain how many times you need to loop, a `while` loop is appropriate:\n",
            "\n",
            "```python\n",
            "count = 0\n",
            "while count < 5:\n",
            "    print(count)\n",
            "    count += 1\n",
            "```\n",
            "\n",
            "Which loop is best depends on your use-case, but in Python, `for` loops are preferred when iterating over sequences, and `while` loops are useful for indefinite loops.\n",
            "\n",
            "If you'd like, I can help you write a loop tailored to your specific task!\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2nxxhB2R6yy"
      },
      "source": [
        "## Task 5: Retrieval Augmented Generation\n",
        "\n",
        "Now we can create a RAG prompt - which will help our system behave in a way that makes sense!\n",
        "\n",
        "There is much you could do here, many tweaks and improvements to be made!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "D1hamzGaR6yy"
      },
      "outputs": [],
      "source": [
        "RAG_SYSTEM_TEMPLATE = \"\"\"You are a knowledgeable assistant that answers questions based strictly on provided context.\n",
        "\n",
        "Instructions:\n",
        "- Only answer questions using information from the provided context\n",
        "- If the context doesn't contain relevant information, respond with \"I don't know\"\n",
        "- Be accurate and cite specific parts of the context when possible\n",
        "- Keep responses {response_style} and {response_length}\n",
        "- Only use the provided context. Do not use external knowledge.\n",
        "- Only provide answers when you are confident the context supports your response.\"\"\"\n",
        "\n",
        "RAG_USER_TEMPLATE = \"\"\"Context Information:\n",
        "{context}\n",
        "\n",
        "Number of relevant sources found: {context_count}\n",
        "{similarity_scores}\n",
        "\n",
        "Question: {user_query}\n",
        "\n",
        "Please provide your answer based solely on the context above.\"\"\"\n",
        "\n",
        "rag_system_prompt = SystemRolePrompt(\n",
        "    RAG_SYSTEM_TEMPLATE,\n",
        "    strict=True,\n",
        "    defaults={\n",
        "        \"response_style\": \"concise\",\n",
        "        \"response_length\": \"brief\"\n",
        "    }\n",
        ")\n",
        "\n",
        "rag_user_prompt = UserRolePrompt(\n",
        "    RAG_USER_TEMPLATE,\n",
        "    strict=True,\n",
        "    defaults={\n",
        "        \"context_count\": \"\",\n",
        "        \"similarity_scores\": \"\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RetrievalAugmentedQAPipeline:\n",
        "    def __init__(self, llm: ChatOpenAI, vector_db_retriever: VectorDatabase, \n",
        "                 response_style: str = \"detailed\", include_scores: bool = False) -> None:\n",
        "        self.llm = llm\n",
        "        self.vector_db_retriever = vector_db_retriever\n",
        "        self.response_style = response_style\n",
        "        self.include_scores = include_scores\n",
        "\n",
        "    def run_pipeline(self, user_query: str, k: int = 4, **system_kwargs) -> dict:\n",
        "        # Retrieve relevant contexts\n",
        "        context_list = self.vector_db_retriever.search_by_text(user_query, k=k)\n",
        "        \n",
        "        context_prompt = \"\"\n",
        "        similarity_scores = []\n",
        "        \n",
        "        for i, (context, score) in enumerate(context_list, 1):\n",
        "            context_prompt += f\"[Source {i}]: {context}\\n\\n\"\n",
        "            similarity_scores.append(f\"Source {i}: {score:.3f}\")\n",
        "        \n",
        "        # Create system message with parameters\n",
        "        system_params = {\n",
        "            \"response_style\": self.response_style,\n",
        "            \"response_length\": system_kwargs.get(\"response_length\", \"detailed\")\n",
        "        }\n",
        "        \n",
        "        formatted_system_prompt = rag_system_prompt.create_message(**system_params)\n",
        "        \n",
        "        user_params = {\n",
        "            \"user_query\": user_query,\n",
        "            \"context\": context_prompt.strip(),\n",
        "            \"context_count\": len(context_list),\n",
        "            \"similarity_scores\": f\"Relevance scores: {', '.join(similarity_scores)}\" if self.include_scores else \"\"\n",
        "        }\n",
        "        \n",
        "        formatted_user_prompt = rag_user_prompt.create_message(**user_params)\n",
        "\n",
        "        return {\n",
        "            \"response\": self.llm.run([formatted_system_prompt, formatted_user_prompt]), \n",
        "            \"context\": context_list,\n",
        "            \"context_count\": len(context_list),\n",
        "            \"similarity_scores\": similarity_scores if self.include_scores else None,\n",
        "            \"prompts_used\": {\n",
        "                \"system\": formatted_system_prompt,\n",
        "                \"user\": formatted_user_prompt\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: The provided context does not explicitly define or mention the term \"Michael Eisner Memorial Weak Executive Problem.\" However, based on the detailed descriptions of Michael Eisner's tenure as Disney's CEO from 1984 to 2003, the following can be inferred about issues related to executive leadership and corporate governance exemplified by his case:\n",
            "\n",
            "1. Eisner's reign illustrates how a powerful CEO can dominate a corporation through personal control, working the board of directors and senior management to maintain and build personal power (Source 1).\n",
            "\n",
            "2. Despite weak corporate performance, declining share prices, and public conflicts with senior executives and suppliers, Eisner managed to hold onto his position and control of Disney for a prolonged period (Source 3).\n",
            "\n",
            "3. His ability to maintain control was partly due to \"innate loyalty biases\" on the part of independent directors and the board, which allowed him to exploit institutional arrangements that typically feature strong managers and weak owners (Source 1 and 3).\n",
            "\n",
            "4. Barry Diller’s quote, \"The Board doesn’t control Disney, and the investors don’t control it. Michael controls it,\" highlights Eisner’s de facto personal control over Disney, underscoring a power imbalance between management and owners (Source 2).\n",
            "\n",
            "Together, these points portray a scenario where an entrenched CEO can exercise substantial discretion and control, often to the detriment of shareholder value and corporate governance, particularly in the absence of strong board intervention. This can be understood as a governance problem involving weak executive oversight paired with strong individual managerial power.\n",
            "\n",
            "Thus, while the context does not specifically name or define the \"Michael Eisner Memorial Weak Executive Problem,\" the case study of Eisner’s leadership of Disney exemplifies a problem where a dominant and hubristic CEO keeps power despite poor corporate performance and weak governance mechanisms, reflecting a \"weak executive\" problem in corporate governance terms.\n",
            "\n",
            "Context Count: 3\n",
            "Similarity Scores: ['Source 1: 0.607', 'Source 2: 0.592', 'Source 3: 0.592']\n"
          ]
        }
      ],
      "source": [
        "rag_pipeline = RetrievalAugmentedQAPipeline(\n",
        "    vector_db_retriever=vector_db,\n",
        "    llm=chat_openai,\n",
        "    response_style=\"detailed\",\n",
        "    include_scores=True\n",
        ")\n",
        "\n",
        "result = rag_pipeline.run_pipeline(\n",
        "    \"What is the 'Michael Eisner Memorial Weak Executive Problem'?\",\n",
        "    k=3,\n",
        "    response_length=\"comprehensive\", \n",
        "    include_warnings=True,\n",
        "    confidence_required=True\n",
        ")\n",
        "\n",
        "print(f\"Response: {result['response']}\")\n",
        "print(f\"\\nContext Count: {result['context_count']}\")\n",
        "print(f\"Similarity Scores: {result['similarity_scores']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZIJI19uR6yz"
      },
      "source": [
        "#### ❓ Question #4:\n",
        "\n",
        "What prompting strategies could you use to make the LLM have a more thoughtful, detailed response?\n",
        "\n",
        "What is that strategy called?\n",
        "\n",
        "> NOTE: You can look through our [OpenAI Responses API](https://colab.research.google.com/drive/14SCfRnp39N7aoOx8ZxadWb0hAqk4lQdL?usp=sharing) notebook for an answer to this question if you get stuck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ✅ Answer:\n",
        "Some prompting strategies for detailed response are: \n",
        "Role Assignment: as we have done in this assignment - with a developer or expert role, it will provide more depth\n",
        "Chain of Thought Prompting: ask for assumption or references, logic and summarization.\n",
        "Few shot prompting: Give examples of the type of answer we need and then ask the question\n",
        "Decomposition prompting: Asking the model to break the problem into subparts like pros, cons , best practices, pitfalls etc\n",
        "Make sure we configure the response type to be comprehensive/detailed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🏗️ Activity #1:\n",
        "\n",
        "Enhance your RAG application in some way! \n",
        "\n",
        "Suggestions are: \n",
        "\n",
        "- Allow it to work with PDF files\n",
        "- Implement a new distance metric\n",
        "- Add metadata support to the vector database\n",
        "- Use a different embedding model\n",
        "\n",
        "While these are suggestions, you should feel free to make whatever augmentations you desire! If you shared an idea during Session 1, think about features you might need to incorporate for your use case! \n",
        "\n",
        "When you're finished making the augments to your RAG application - vibe check it against the old one - see if you can \"feel the improvement\"!\n",
        "\n",
        "> NOTE: These additions might require you to work within the `aimakerspace` library - that's expected!\n",
        "\n",
        "> NOTE: If you're not sure where to start - ask Cursor (CMD/CTRL+L) to guide you through the changes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG Performance Comparison: Text vs Text + PDF\n",
        "\n",
        "Let's demonstrate the impact of adding PDF data to our RAG system by comparing results before and after augmentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "RAG PIPELINE COMPARISON: TEXT vs TEXT + PDF\n",
            "================================================================================\n",
            "\n",
            "📄 PHASE 1: RAG Pipeline with TEXT DATA ONLY\n",
            "--------------------------------------------------\n",
            "✅ Loaded 1 text document(s)\n",
            "✅ Created 373 text chunks\n",
            "🔄 Building vector database with text data only...\n",
            "✅ Vector database built with 373 vectors\n",
            "\n",
            "🔍 Query: 'What is the Michael Eisner Memorial Weak Executive Problem?'\n",
            "\n",
            "RAG Pipeline Results with TEXT-ONLY data:\n",
            "Response: The Michael Eisner Memorial Weak Executive Problem refers to the tendency of CEOs or startup founders to hire weak executives for the very function or specialty that brought them to their leadership position. This happens because the CEO or founder has a hard time letting go of that function and wants to continue being the primary authority or expert in that area. The example given is Michael Eisner, who was a brilliant TV network executive before becoming CEO of Disney; after buying ABC, the network fell to fourth place, and Eisner expressed a desire to personally turn it around if he had extra time, but he did not. This problem results in the CEO hiring a weak executive in that function so that the CEO can remain \"the man,\" effectively holding onto control rather than empowering a strong executive to take charge (Source 1).\n",
            "Context sources: 5\n",
            "Similarity scores: ['Source 1: 0.560', 'Source 2: 0.523', 'Source 3: 0.485']\n",
            "\n",
            "\n",
            "📄📄 PHASE 2: AUGMENTING DATABASE WITH PDF DATA\n",
            "--------------------------------------------------\n",
            "✅ Loaded 1 PDF document(s)\n",
            "✅ Created 89 PDF chunks\n",
            "🔄 Augmenting existing vector database with PDF data...\n",
            "✅ Augmented database now has 462 vectors\n",
            "   (Original: 373 + Added: 89)\n",
            "\n",
            "🔍 Query: 'What is the Michael Eisner Memorial Weak Executive Problem?'\n",
            "\n",
            "RAG Pipeline Results with AUGMENTED data:\n",
            "Response: The \"Michael Eisner Memorial Weak Executive Problem\" refers to a phenomenon where a CEO, often one who rose through a particular functional area of a company (such as product management, sales, or marketing), hires a deliberately weak executive to run that former specialty. This is done so that the CEO can maintain control and continue to be \"the man\" in that function, rather than fully delegating it to a strong subordinate. The problem was named after Michael Eisner, the former Disney CEO who had previously been a brilliant TV network executive. When Eisner bought ABC at Disney, ABC promptly fell to fourth place, and Eisner responded by saying, \"If I had an extra two days a week, I could turn around ABC myself.\" However, he did not have that extra time, illustrating the difficulty a CEO has in letting go of the function that brought him to the party and the consequent tendency to hire weak executives in that area (Source 2).\n",
            "Context sources: 5\n",
            "Similarity scores: ['Source 1: 0.578', 'Source 2: 0.560', 'Source 3: 0.553']\n",
            "\n",
            "\n",
            "📊 COMPARISON SUMMARY\n",
            "==================================================\n",
            "Additional vectors from PDF: 0\n",
            "\n",
            "RAG Pipeline Performance:\n",
            "Text-only sources:  5\n",
            "Augmented sources:  5\n",
            "🎯 Top similarity score improvement: +0.018 (✅ Better)\n",
            "\n",
            "💡 Key Insight: This shows how you can incrementally improve your RAG system\n",
            "   by adding new data sources without rebuilding from scratch!\n"
          ]
        }
      ],
      "source": [
        "# Demo: RAG Pipeline Performance Comparison\n",
        "import asyncio\n",
        "from aimakerspace.openai_utils.embedding import EmbeddingModel\n",
        "from aimakerspace.vectordatabase import VectorDatabase\n",
        "from aimakerspace.text_utils import TextFileLoader, PDFFileLoader, CharacterTextSplitter\n",
        "\n",
        "# Test query\n",
        "test_query = \"What is the Michael Eisner Memorial Weak Executive Problem?\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"RAG PIPELINE COMPARISON: TEXT vs TEXT + PDF\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# PHASE 1: RAG Pipeline with TEXT DATA ONLY\n",
        "# ============================================================================\n",
        "print(\"\\n📄 PHASE 1: RAG Pipeline with TEXT DATA ONLY\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Load only text data\n",
        "text_loader = TextFileLoader(\"data/PMarcaBlogs.txt\")\n",
        "text_documents = text_loader.load_documents()\n",
        "print(f\"✅ Loaded {len(text_documents)} text document(s)\")\n",
        "\n",
        "# Chunk text data\n",
        "text_splitter = CharacterTextSplitter()\n",
        "text_chunks = text_splitter.split_texts(text_documents)\n",
        "print(f\"✅ Created {len(text_chunks)} text chunks\")\n",
        "\n",
        "# Create vector database with text data only\n",
        "embed_model = EmbeddingModel(\n",
        "    embeddings_model_name=\"text-embedding-3-large\",\n",
        "    dimensions=1024\n",
        ")\n",
        "vector_db_text_only = VectorDatabase(embedding_model=embed_model)\n",
        "\n",
        "print(\"🔄 Building vector database with text data only...\")\n",
        "vector_db_text_only = asyncio.run(vector_db_text_only.abuild_from_list(text_chunks))\n",
        "print(f\"✅ Vector database built with {len(vector_db_text_only.vectors)} vectors\")\n",
        "\n",
        "# Create RAG pipeline with text-only data\n",
        "rag_pipeline_text_only = RetrievalAugmentedQAPipeline(\n",
        "    vector_db_retriever=vector_db_text_only,\n",
        "    llm=chat_openai,\n",
        "    response_style=\"detailed\",\n",
        "    include_scores=True\n",
        ")\n",
        "\n",
        "print(f\"\\n🔍 Query: '{test_query}'\")\n",
        "print(\"\\nRAG Pipeline Results with TEXT-ONLY data:\")\n",
        "text_result = rag_pipeline_text_only.run_pipeline(\n",
        "    test_query,\n",
        "    k=5,\n",
        "    response_length=\"comprehensive\", \n",
        "    include_warnings=True,\n",
        "    confidence_required=True\n",
        ")\n",
        "print(f\"Response: {text_result['response']}\")\n",
        "print(f\"Context sources: {text_result['context_count']}\")\n",
        "print(f\"Similarity scores: {text_result['similarity_scores'][:3] if text_result['similarity_scores'] else 'N/A'}\")\n",
        "\n",
        "# ============================================================================\n",
        "# PHASE 2: AUGMENT EXISTING DATABASE WITH PDF DATA\n",
        "# ============================================================================\n",
        "print(\"\\n\\n📄📄 PHASE 2: AUGMENTING DATABASE WITH PDF DATA\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Load PDF data\n",
        "pdf_loader = PDFFileLoader(\"data/2A_Forbes.pdf\")\n",
        "pdf_documents = pdf_loader.load_documents()\n",
        "print(f\"✅ Loaded {len(pdf_documents)} PDF document(s)\")\n",
        "\n",
        "# Chunk PDF data\n",
        "pdf_chunks = text_splitter.split_texts(pdf_documents)\n",
        "print(f\"✅ Created {len(pdf_chunks)} PDF chunks\")\n",
        "\n",
        "# Augment the existing vector database with PDF data\n",
        "print(\"🔄 Augmenting existing vector database with PDF data...\")\n",
        "vector_db_augmented = asyncio.run(vector_db_text_only.abuild_from_list(pdf_chunks))\n",
        "print(f\"✅ Augmented database now has {len(vector_db_augmented.vectors)} vectors\")\n",
        "print(f\"   (Original: {len(text_chunks)} + Added: {len(pdf_chunks)})\")\n",
        "\n",
        "# Create RAG pipeline with augmented data\n",
        "rag_pipeline_augmented = RetrievalAugmentedQAPipeline(\n",
        "    vector_db_retriever=vector_db_augmented,\n",
        "    llm=chat_openai,\n",
        "    response_style=\"detailed\",\n",
        "    include_scores=True\n",
        ")\n",
        "\n",
        "print(f\"\\n🔍 Query: '{test_query}'\")\n",
        "print(\"\\nRAG Pipeline Results with AUGMENTED data:\")\n",
        "augmented_result = rag_pipeline_augmented.run_pipeline(\n",
        "    test_query,\n",
        "    k=5,\n",
        "    response_length=\"comprehensive\", \n",
        "    include_warnings=True,\n",
        "    confidence_required=True\n",
        ")\n",
        "print(f\"Response: {augmented_result['response']}\")\n",
        "print(f\"Context sources: {augmented_result['context_count']}\")\n",
        "print(f\"Similarity scores: {augmented_result['similarity_scores'][:3] if augmented_result['similarity_scores'] else 'N/A'}\")\n",
        "\n",
        "# ============================================================================\n",
        "# COMPARISON SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\\n📊 COMPARISON SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Additional vectors from PDF: {len(vector_db_augmented.vectors) - len(vector_db_text_only.vectors)}\")\n",
        "\n",
        "print(f\"\\nRAG Pipeline Performance:\")\n",
        "print(f\"Text-only sources:  {text_result['context_count']}\")\n",
        "print(f\"Augmented sources:  {augmented_result['context_count']}\")\n",
        "\n",
        "# Compare top similarity scores\n",
        "text_top_score = float(text_result['similarity_scores'][0].split(': ')[1]) if text_result['similarity_scores'] else 0\n",
        "augmented_top_score = float(augmented_result['similarity_scores'][0].split(': ')[1]) if augmented_result['similarity_scores'] else 0\n",
        "score_improvement = augmented_top_score - text_top_score\n",
        "print(f\"🎯 Top similarity score improvement: {score_improvement:+.3f} ({'✅ Better' if score_improvement > 0 else '❌ No change' if score_improvement == 0 else '⚠️ Worse'})\")\n",
        "\n",
        "print(f\"\\n💡 Key Insight: This shows how you can incrementally improve your RAG system\")\n",
        "print(f\"   by adding new data sources without rebuilding from scratch!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedding Model Comparison: Large vs Small\n",
        "\n",
        "Let's compare how different embedding models affect RAG pipeline performance. We'll test the same queries with both the large model (1024 dimensions) and small model (1536 dimensions).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EMBEDDING MODEL COMPARISON: LARGE vs SMALL\n",
            "================================================================================\n",
            "\n",
            "🔬 TESTING LARGE MODEL (text-embedding-3-large, 1024 dims)\n",
            "------------------------------------------------------------\n",
            "🔄 Building vector database with large model...\n",
            "✅ Large model database built with 462 vectors\n",
            "\n",
            "🔍 Query: 'What is the Michael Eisner Memorial Weak Executive Problem?'\n",
            "Large Model Results:\n",
            "Response: The \"Michael Eisner Memorial Weak Executive Problem\" refers to a phenomenon where a CEO, having risen through a particular functional area, intentionally hires a weak executive to run that same function in their organization. This is done so the CEO can retain personal control and continue to dominate that area, rather than delegating it effectively to a strong executive. The term is named after Michael Eisner, former CEO of Disney, who had been a brilliant TV network executive but, after acquiring ABC for Disney, saw the network fall to fourth place. Instead of empowering a strong executive, Eisner reportedly suggested that if he had extra time, he would turn around ABC himself, which he did not have. This example highlights how CEOs or founders may struggle to let go of the function that brought them success and consequently appoint weaker executives to maintain authority in that domain (Source 2).\n",
            "Context sources: 5\n",
            "Top similarity scores: ['Source 1: 0.578', 'Source 2: 0.560', 'Source 3: 0.552']\n",
            "\n",
            "\n",
            "🔬 TESTING SMALL MODEL (text-embedding-3-small, 1536 dims)\n",
            "------------------------------------------------------------\n",
            "🔄 Building vector database with small model...\n",
            "✅ Small model database built with 462 vectors\n",
            "\n",
            "🔍 Query: 'What is the Michael Eisner Memorial Weak Executive Problem?'\n",
            "Small Model Results:\n",
            "Response: The \"Michael Eisner Memorial Weak Executive Problem\" refers to a situation where a CEO, particularly one who previously excelled in a specific function (such as product management, sales, or marketing), intentionally hires a weak executive to run that same function so that the CEO can continue to maintain control and be \"the man\" in that area. This problem is named after Michael Eisner, the former CEO of Disney, who was previously a brilliant TV network executive. When Eisner's Disney bought ABC, ABC promptly fell to fourth place. Eisner responded by saying that if he had an extra two days a week, he could turn around ABC himself. However, he did not have that extra time. This illustrates how CEOs may struggle to relinquish control over the functions that brought them success, leading them to appoint weaker executives to keep their personal dominance intact (Source 2).\n",
            "Context sources: 5\n",
            "Top similarity scores: ['Source 1: 0.666', 'Source 2: 0.654', 'Source 3: 0.619']\n",
            "\n",
            "\n",
            "📊 DETAILED COMPARISON\n",
            "==================================================\n",
            "Model Specifications:\n",
            "  Large:  text-embedding-3-large  (1024 dimensions)\n",
            "  Small:  text-embedding-3-small  (1536 dimensions)\n",
            "\n",
            "Performance Metrics:\n",
            "  Large model top score:  0.578\n",
            "  Small model top score:  0.666\n",
            "  Score difference:       -0.088\n",
            "\n",
            "🎯 Analysis:\n",
            "  ✅ Small model performs better (score difference: -0.088)\n",
            "\n",
            "💡 Key Insights:\n",
            "  • Large model (1024 dims): More focused, potentially better for specific queries\n",
            "  • Small model (1536 dims): More general, potentially better for diverse queries\n",
            "  • Dimension count doesn't always correlate with performance\n",
            "  • Choose based on your specific use case and data characteristics\n"
          ]
        }
      ],
      "source": [
        "# Embedding Model Performance Comparison\n",
        "import asyncio\n",
        "from aimakerspace.openai_utils.embedding import EmbeddingModel\n",
        "from aimakerspace.vectordatabase import VectorDatabase\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EMBEDDING MODEL COMPARISON: LARGE vs SMALL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Test query\n",
        "test_query = \"What is the Michael Eisner Memorial Weak Executive Problem?\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# LARGE MODEL (text-embedding-3-large, 1024 dimensions)\n",
        "# ============================================================================\n",
        "print(\"\\n🔬 TESTING LARGE MODEL (text-embedding-3-large, 1024 dims)\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Create large model\n",
        "large_embed_model = EmbeddingModel(\n",
        "    embeddings_model_name=\"text-embedding-3-large\",\n",
        "    dimensions=1024\n",
        ")\n",
        "vector_db_large = VectorDatabase(embedding_model=large_embed_model)\n",
        "\n",
        "print(\"🔄 Building vector database with large model...\")\n",
        "vector_db_large = asyncio.run(vector_db_large.abuild_from_list(split_documents))\n",
        "print(f\"✅ Large model database built with {len(vector_db_large.vectors)} vectors\")\n",
        "\n",
        "# Create RAG pipeline with large model\n",
        "rag_pipeline_large = RetrievalAugmentedQAPipeline(\n",
        "    vector_db_retriever=vector_db_large,\n",
        "    llm=chat_openai,\n",
        "    response_style=\"detailed\",\n",
        "    include_scores=True\n",
        ")\n",
        "\n",
        "print(f\"\\n🔍 Query: '{test_query}'\")\n",
        "print(\"Large Model Results:\")\n",
        "large_result = rag_pipeline_large.run_pipeline(\n",
        "    test_query,\n",
        "    k=5,\n",
        "    response_length=\"comprehensive\", \n",
        "    include_warnings=True,\n",
        "    confidence_required=True\n",
        ")\n",
        "print(f\"Response: {large_result['response']}\")\n",
        "print(f\"Context sources: {large_result['context_count']}\")\n",
        "print(f\"Top similarity scores: {large_result['similarity_scores'][:3] if large_result['similarity_scores'] else 'N/A'}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SMALL MODEL (text-embedding-3-small, 1536 dimensions)\n",
        "# ============================================================================\n",
        "print(\"\\n\\n🔬 TESTING SMALL MODEL (text-embedding-3-small, 1536 dims)\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Create small model\n",
        "small_embed_model = EmbeddingModel(\n",
        "    embeddings_model_name=\"text-embedding-3-small\",\n",
        "    dimensions=None  # Use default 1536 dimensions\n",
        ")\n",
        "vector_db_small = VectorDatabase(embedding_model=small_embed_model)\n",
        "\n",
        "print(\"🔄 Building vector database with small model...\")\n",
        "vector_db_small = asyncio.run(vector_db_small.abuild_from_list(split_documents))\n",
        "print(f\"✅ Small model database built with {len(vector_db_small.vectors)} vectors\")\n",
        "\n",
        "# Create RAG pipeline with small model\n",
        "rag_pipeline_small = RetrievalAugmentedQAPipeline(\n",
        "    vector_db_retriever=vector_db_small,\n",
        "    llm=chat_openai,\n",
        "    response_style=\"detailed\",\n",
        "    include_scores=True\n",
        ")\n",
        "\n",
        "print(f\"\\n🔍 Query: '{test_query}'\")\n",
        "print(\"Small Model Results:\")\n",
        "small_result = rag_pipeline_small.run_pipeline(\n",
        "    test_query,\n",
        "    k=5,\n",
        "    response_length=\"comprehensive\", \n",
        "    include_warnings=True,\n",
        "    confidence_required=True\n",
        ")\n",
        "print(f\"Response: {small_result['response']}\")\n",
        "print(f\"Context sources: {small_result['context_count']}\")\n",
        "print(f\"Top similarity scores: {small_result['similarity_scores'][:3] if small_result['similarity_scores'] else 'N/A'}\")\n",
        "\n",
        "# ============================================================================\n",
        "# DETAILED COMPARISON\n",
        "# ============================================================================\n",
        "print(\"\\n\\n📊 DETAILED COMPARISON\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Compare top similarity scores\n",
        "large_top_score = float(large_result['similarity_scores'][0].split(': ')[1]) if large_result['similarity_scores'] else 0\n",
        "small_top_score = float(small_result['similarity_scores'][0].split(': ')[1]) if small_result['similarity_scores'] else 0\n",
        "score_difference = large_top_score - small_top_score\n",
        "\n",
        "print(f\"Model Specifications:\")\n",
        "print(f\"  Large:  text-embedding-3-large  (1024 dimensions)\")\n",
        "print(f\"  Small:  text-embedding-3-small  (1536 dimensions)\")\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  Large model top score:  {large_top_score:.3f}\")\n",
        "print(f\"  Small model top score:  {small_top_score:.3f}\")\n",
        "print(f\"  Score difference:       {score_difference:+.3f}\")\n",
        "\n",
        "\n",
        "print(f\"\\n🎯 Analysis:\")\n",
        "if score_difference > 0.01:\n",
        "    print(f\"  ✅ Large model performs better (score difference: +{score_difference:.3f})\")\n",
        "elif score_difference < -0.01:\n",
        "    print(f\"  ✅ Small model performs better (score difference: {score_difference:.3f})\")\n",
        "else:\n",
        "    print(f\"  ⚖️  Both models perform similarly (score difference: {score_difference:.3f})\")\n",
        "\n",
        "print(f\"\\n💡 Key Insights:\")\n",
        "print(f\"  • Large model (1024 dims): More focused, potentially better for specific queries\")\n",
        "print(f\"  • Small model (1536 dims): More general, potentially better for diverse queries\")\n",
        "print(f\"  • Dimension count doesn't always correlate with performance\")\n",
        "print(f\"  • Choose based on your specific use case and data characteristics\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "MULTI-QUERY EMBEDDING MODEL COMPARISON\n",
            "================================================================================\n",
            "Testing both models across multiple queries for comprehensive comparison...\n",
            "\n",
            "🔍 QUERY 1: 'What is the Michael Eisner Memorial Weak Executive Problem?'\n",
            "------------------------------------------------------------\n",
            "📊 Large Model:  0.578 (sources: 6)\n",
            "📊 Small Model:  0.666 (sources: 6)\n",
            "📈 Difference:   -0.088 (Small better)\n",
            "\n",
            "🔍 QUERY 2: 'What are the characteristics of weak executive leadership?'\n",
            "------------------------------------------------------------\n",
            "📊 Large Model:  0.557 (sources: 6)\n",
            "📊 Small Model:  0.570 (sources: 6)\n",
            "📈 Difference:   -0.013 (Small better)\n",
            "\n",
            "🔍 QUERY 3: 'How does board loyalty bias affect corporate governance?'\n",
            "------------------------------------------------------------\n",
            "📊 Large Model:  0.740 (sources: 6)\n",
            "📊 Small Model:  0.691 (sources: 6)\n",
            "📈 Difference:   +0.049 (Large better)\n",
            "\n",
            "🔍 QUERY 4: 'What are the consequences of destructive corporate leadership?'\n",
            "------------------------------------------------------------\n",
            "📊 Large Model:  0.677 (sources: 6)\n",
            "📊 Small Model:  0.722 (sources: 6)\n",
            "📈 Difference:   -0.045 (Small better)\n",
            "\n",
            "\n",
            "📊 OVERALL PERFORMANCE SUMMARY\n",
            "==================================================\n",
            "Average Performance:\n",
            "  Large model:  0.638\n",
            "  Small model:  0.662\n",
            "  Difference:   -0.024\n",
            "\n",
            "Query-by-Query Results:\n",
            "  Large model wins:  1/4 queries\n",
            "  Small model wins:  3/4 queries\n",
            "  Ties:              0/4 queries\n",
            "\n",
            "🎯 Final Verdict:\n",
            "  🏆 SMALL MODEL WINS! (Average advantage: +0.024)\n",
            "     The small model with 1536 dimensions performs better for this dataset.\n",
            "\n",
            "💡 Practical Recommendations:\n",
            "  • For production: Choose the model with better average performance\n",
            "  • For cost optimization: Small model is cheaper to run\n",
            "  • For accuracy: Large model may be better for specific domains\n",
            "  • For general use: Test both with your specific data and queries\n"
          ]
        }
      ],
      "source": [
        "# Multi-Query Embedding Model Comparison\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"MULTI-QUERY EMBEDDING MODEL COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Test multiple queries to get comprehensive comparison\n",
        "test_queries = [\n",
        "    \"What is the Michael Eisner Memorial Weak Executive Problem?\",\n",
        "    \"What are the characteristics of weak executive leadership?\",\n",
        "    \"How does board loyalty bias affect corporate governance?\",\n",
        "    \"What are the consequences of destructive corporate leadership?\"\n",
        "]\n",
        "\n",
        "print(\"Testing both models across multiple queries for comprehensive comparison...\")\n",
        "\n",
        "large_scores = []\n",
        "small_scores = []\n",
        "\n",
        "for query_idx, query in enumerate(test_queries, 1):\n",
        "    print(f\"\\n🔍 QUERY {query_idx}: '{query}'\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Test with large model\n",
        "    large_result = rag_pipeline_large.run_pipeline(\n",
        "        query,\n",
        "        k=6,\n",
        "        response_length=\"detailed\", \n",
        "        include_warnings=False,\n",
        "        confidence_required=True\n",
        "    )\n",
        "    \n",
        "    # Test with small model\n",
        "    small_result = rag_pipeline_small.run_pipeline(\n",
        "        query,\n",
        "        k=6,\n",
        "        response_length=\"detailed\", \n",
        "        include_warnings=False,\n",
        "        confidence_required=True\n",
        "    )\n",
        "    \n",
        "    # Extract top scores\n",
        "    large_score = float(large_result['similarity_scores'][0].split(': ')[1]) if large_result['similarity_scores'] else 0\n",
        "    small_score = float(small_result['similarity_scores'][0].split(': ')[1]) if small_result['similarity_scores'] else 0\n",
        "    \n",
        "    large_scores.append(large_score)\n",
        "    small_scores.append(small_score)\n",
        "    \n",
        "    # Show results\n",
        "    print(f\"📊 Large Model:  {large_score:.3f} (sources: {large_result['context_count']})\")\n",
        "    print(f\"📊 Small Model:  {small_score:.3f} (sources: {small_result['context_count']})\")\n",
        "    \n",
        "    difference = large_score - small_score\n",
        "    print(f\"📈 Difference:   {difference:+.3f} ({'Large better' if difference > 0 else 'Small better' if difference < 0 else 'Tie'})\")\n",
        "\n",
        "# Overall comparison\n",
        "print(f\"\\n\\n📊 OVERALL PERFORMANCE SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "avg_large_score = sum(large_scores) / len(large_scores)\n",
        "avg_small_score = sum(small_scores) / len(small_scores)\n",
        "overall_difference = avg_large_score - avg_small_score\n",
        "\n",
        "print(f\"Average Performance:\")\n",
        "print(f\"  Large model:  {avg_large_score:.3f}\")\n",
        "print(f\"  Small model:  {avg_small_score:.3f}\")\n",
        "print(f\"  Difference:   {overall_difference:+.3f}\")\n",
        "\n",
        "# Count wins\n",
        "large_wins = sum(1 for l, s in zip(large_scores, small_scores) if l > s)\n",
        "small_wins = sum(1 for l, s in zip(large_scores, small_scores) if s > l)\n",
        "ties = len(large_scores) - large_wins - small_wins\n",
        "\n",
        "print(f\"\\nQuery-by-Query Results:\")\n",
        "print(f\"  Large model wins:  {large_wins}/{len(test_queries)} queries\")\n",
        "print(f\"  Small model wins:  {small_wins}/{len(test_queries)} queries\")\n",
        "print(f\"  Ties:              {ties}/{len(test_queries)} queries\")\n",
        "\n",
        "print(f\"\\n🎯 Final Verdict:\")\n",
        "if overall_difference > 0.01:\n",
        "    print(f\"  🏆 LARGE MODEL WINS! (Average advantage: +{overall_difference:.3f})\")\n",
        "    print(f\"     The large model with 1024 dimensions performs better for this dataset.\")\n",
        "elif overall_difference < -0.01:\n",
        "    print(f\"  🏆 SMALL MODEL WINS! (Average advantage: +{abs(overall_difference):.3f})\")\n",
        "    print(f\"     The small model with 1536 dimensions performs better for this dataset.\")\n",
        "else:\n",
        "    print(f\"  🤝 IT'S A TIE! (Difference: {overall_difference:.3f})\")\n",
        "    print(f\"     Both models perform similarly for this dataset.\")\n",
        "\n",
        "print(f\"\\n💡 Practical Recommendations:\")\n",
        "print(f\"  • For production: Choose the model with better average performance\")\n",
        "print(f\"  • For cost optimization: Small model is cheaper to run\")\n",
        "print(f\"  • For accuracy: Large model may be better for specific domains\")\n",
        "print(f\"  • For general use: Test both with your specific data and queries\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
